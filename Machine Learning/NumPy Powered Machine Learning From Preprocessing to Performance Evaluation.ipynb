{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"9BPzS0g9Ox7-"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"svj3G7hj4ZxY"},"source":["In this homework, you will continue to use the same dataset you have used in the first lab, as well as the code you have made. However, this time, you will create a model and even test it - by dividing the dataset into training and test sets. You will practice applying each function one after the other in a correct order. \n","\n","To practice NumPy efficiently, unlike the lab, you will only use the values of the dataset, ignoring Pandas properties such as column names.\n","\n","- Run the block below to load the dataset (**X** and **y**)."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2HAUc00Wmu0E"},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","\n","california_housing = fetch_california_housing(as_frame=False)\n","\n","X = california_housing.data\n","y = california_housing.target"]},{"cell_type":"markdown","metadata":{"id":"-txQY1eUoWeg"},"source":["Now, you are ready to use both features and labels. To handle it correctly, you need to be familiar with its axis concept as it no longer has indices and columns that you can check by printing the variable."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"u4KbMNxMm9Oi"},"outputs":[{"data":{"text/plain":["array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n","          37.88      , -122.23      ],\n","       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n","          37.86      , -122.22      ],\n","       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n","          37.85      , -122.24      ],\n","       ...,\n","       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n","          39.43      , -121.22      ],\n","       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n","          39.43      , -121.32      ],\n","       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n","          39.37      , -121.24      ]])"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["X"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9rTO_NYMNHlg"},"outputs":[{"data":{"text/plain":["array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["y"]},{"cell_type":"markdown","metadata":{"id":"Fbj_IpkHOx8A"},"source":["### 1. Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"EE0ALwTyOx8A"},"source":["The first task is to open the dataset and preprocess it into the form that the model can understand. It involves imputation, train_test_split, standardization, and normalization. Some functions are already covered by the first lab, so if you finished the lab before, you can freely bring your code here to finish your homework.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"t2uFZSUuoh9t"},"source":["First, you need to develop both standardization and normalization functions. You can re-use your lab functions here if you have finished your lab tasks. Please carefully refer to the definitions of those two functions as follows:\n","\n","\n","- Standardization: Make features have the same standard deviaton and mean.\n","\n","- Normalization: Make the range of value normalized into [0, 1]. This means that each column's minimum value should be zero and maximum value should be one."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"bQRVVievOx8A"},"outputs":[],"source":["def standardize(data):\n","  \"\"\"\n","  Input: NumPy ndarray\n","  Output: NumPy ndarray with column mean == 0 and std == 1\n","  \"\"\"\n","  return (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n","\n","def normalize(data):\n","  \"\"\"\"\n","  Input: NumPy ndarray\n","  Output: NumPy ndarray with column min == 0 and max == 1\n","  \"\"\"\n","  return (data - np.min(data, axis=0)) / (np.max(data, axis=0) - np.min(data, axis=0))\n","\n","def standardize(data):\n","  \"\"\"\n","  Input: Pandas DataFrame\n","  Output: Pandas DataFrame with mean == 0 and std == 1\n","  \"\"\"\n","  mean = np.mean(data, axis=0)\n","  std = np.std(data, axis=0)\n","  data_new = (data - mean) / std\n","  return data_new\n","\n","def normalize(data):\n","  \"\"\"\"\n","  Input: Pandas DataFrame\n","  Output: Pandas DataFrame with min == 0 and max == 1\n","  \"\"\"\n","\n","  return (data - np.min(data, axis=0))/(np.max(data, axis=0) - np.min(data, axis=0))\n"]},{"cell_type":"markdown","metadata":{"id":"Cb4WgGQ0o_9G"},"source":["Let's apply both functions separately and create X_standardized and X_normalized."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2qf7fOZG54MA"},"outputs":[],"source":["X_standardized = standardize(X)\n","X_normalized = normalize(X)"]},{"cell_type":"markdown","metadata":{"id":"So46vKNl53KL"},"source":["You may also need to check if those functions are correctly made. Create a function to check the dataset's min, max, mean, std of each feature. You can re-use your lab function (**describe**) but this time you are not allowed to use Pandas DataFrame. There is no expected format for this function if you are successfully able to plot four statistics (min, max, mean, std). "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"l9BqVsG36Tsl"},"outputs":[],"source":["def describe(data):\n","  \"\"\"\n","  Describe four statistics of the dataset.\n","  \n","  Input: NumPy ndarray\n","  Output: vertical min, max, mean, standard deviation\n","  \"\"\"\n","  \n","  print(\"Min: \", np.min(data, axis=0))\n","  print(\"Max: \", np.max(data, axis=0))\n","  print(\"Mean: \", np.mean(data, axis=0))\n","  print(\"Std: \", np.std(data, axis=0))"]},{"cell_type":"markdown","metadata":{"id":"FZEPPjpJ6W3B"},"source":["Using this function, let's check if your **standardize** and **normalize** functions are correctly working. \n","- **Your output should be the same as the one below.**"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NY6wtki6c84","outputId":"945364b8-7158-4052-a701-a279cfef5ea1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [-1.77429947 -2.19618048 -1.8523186  -1.61076772 -1.25612255 -0.22899997\n"," -1.447568   -2.38599234]\n","Max:  [  5.85828581   1.85618152  55.16323628  69.57171326  30.25033022\n"," 119.41910319   2.95806762   2.62528006]\n","Mean:  [ 6.60969987e-17  5.50808322e-18  6.60969987e-17 -1.06030602e-16\n"," -1.10161664e-17  3.44255201e-18 -1.07958431e-15 -8.52651283e-15]\n","Std:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"]}],"source":["describe(X_standardized)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G7-p5vdl3H4F","outputId":"b179dea5-39dd-4eab-8b1e-ade4f6160794"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [0. 0. 0. 0. 0. 0. 0. 0.]\n","Max:  [1. 1. 1. 1. 1. 1. 1. 1.]\n","Mean:  [0.23246376 0.54195071 0.03248795 0.02262871 0.03986874 0.00191395\n"," 0.32857188 0.47612505]\n","Std:  [0.13101721 0.24676966 0.01753907 0.0140484  0.03173953 0.00835784\n"," 0.226982   0.19955012]\n"]}],"source":["describe(X_normalized)"]},{"cell_type":"markdown","metadata":{"id":"mj47zZ264Az0"},"source":["However, this is not a complete setting, as you need to both train the model and test it. That means you need to divide the dataset into two parts: {a training set, a test set} and only use the training set to train the model. This means that you also need to create the function for it."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ORFET61FbZVy"},"outputs":[],"source":["def train_test_split(X, y, test_ratio = 0.3, shuffle=True):\n","  # simulation\n","  # cross-val\n","  \n","  \"\"\"\n","  Input:\n","    - X: a set of features\n","    - y: corresponding labels\n","    - test_ratio: ratio of the test set\n","    \n","  Output:\n","    - X_train: separated training instances\n","    - X_test: separated test instances\n","    - y_train: separated training labels\n","    - y_test: separated test labels\n","  \n","  1. Randomly shuffle the indices of the data instances if the shuffle option is True.\n","  2. Divide the indices into two parts with the ratio of [1-test ratio:test ratio]\n","  3. Select training instances and labels with the first set of indices and test instances and labels with the second set of indices\n","  4. Return the training set and the test set\n","  \"\"\"\n","  num_instances = X.shape[0]\n","  indices = np.arange(num_instances)\n","  np.random.shuffle(indices)\n","\n","  split_index = int(test_ratio * num_instances)\n","  train_indices = indices[split_index:]\n","  test_indices = indices[:split_index]\n","\n","  X_train = X[train_indices]\n","  y_train = y[train_indices]\n","  X_test = X[test_indices]\n","  y_test = y[test_indices]\n","\n","  return X_train, X_test, y_train, y_test"]},{"cell_type":"markdown","metadata":{"id":"twOo6_yniUGO"},"source":["Split your dataset into training and test sets with `test ratio = 0.2`."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"f-iSb7IoijO1"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_ratio=0.3, shuffle= True)"]},{"cell_type":"markdown","metadata":{"id":"d9PK6RGbkK7O"},"source":["After applying your train_test_split function, you can check the shape of each subset. The training set should have 14,448 rows while the test set might have 6,192 records. Uncommend the below line and check the shapes."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"bk_pMMo-iquK"},"outputs":[{"data":{"text/plain":["((14448, 8), (6192, 8), (14448,), (6192,))"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{"id":"7T8JBhsTj00O"},"source":["You may remember, when you apply standardization or normalization on both training and test sets, you should not use any statistics from the test set. This means that you should use mean and standard deviation (or max and min values) of the training set and use those statistics to avoid cheating and make a valid model. \n","\n","- Create two functions (**apply_standardization**, **apply_normalization**) that uses training set's statistics and apply standardization or normalization to both sets."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XLs3P181kYHA"},"outputs":[],"source":["def apply_standardization(X_train, X_test):\n","  \"\"\"\n","  Input:\n","    - X_train: training instances\n","    - X_test: test instances\n","\n","  Output:\n","    - X_train_standardized\n","    - X_test_standardized\n","\n","  Use training set's mean and standard deviation to standardize both training and test sets\n","  \"\"\"\n","  mean= np.mean(X_train, axis=0)\n","  std= np.std(X_train, axis=0)\n","  X_train_standardize= (X_train - mean) / std\n","  X_test_standardize= (X_test - mean)/ std\n","  return X_train_standardize, X_test_standardize"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"sOx7Kd221jRq"},"outputs":[],"source":["def apply_normalization(X_train, X_test):\n","  \"\"\"\n","  Input:\n","    - X_train\n","    - X_test\n","\n","  Output:\n","    - X_train_standardized\n","    - X_test_standardized\n","  \"\"\"\n","  X_train_normalized= (X_train - np.min(X_train, axis=0))/(np.max(X_train, axis=0) - np.min(X_train, axis=0))\n","  X_test_normalized= (X_test - np.min(X_test, axis=0))/(np.max(X_test, axis=0) - np.min(X_test, axis=0))\n","  return X_train_normalized, X_test_normalized"]},{"cell_type":"markdown","metadata":{"id":"ujWQ84ZFCa10"},"source":["- Apply two functions (**apply_standardization**, **apply_normalization**) to created standardized and normalized datasets."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"Cw8tjyoCtnqI"},"outputs":[],"source":["X_train_standardized, X_test_standardized = apply_standardization(X_train, X_test)\n","X_train_normalized, X_test_normalized = apply_normalization(X_train, X_test)"]},{"cell_type":"markdown","metadata":{"id":"efxhH-DvCxBx"},"source":["Check the statistics using describe method. Test set should **NOT** have zero mean and standard deviation 1 or zero min and one max. Good test set however might show close value to zero or one."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ch5zQTDDDugQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [-1.76137951 -2.18523489 -1.73339187 -1.49291423 -1.26240461 -0.1964004\n"," -1.45271396 -2.35737283]\n","Max:  [  5.80880117   1.86027551  51.49200809  64.3592303   24.04236311\n"," 100.01325086   2.95419426   2.62502868]\n","Mean:  [-1.02053119e-14  3.28886666e-18 -1.01011642e-14 -1.30075099e-14\n"," -9.64567727e-17  2.65737952e-16 -5.48548840e-14 -2.60026942e-13]\n","Std:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"]}],"source":["describe(X_train_standardized)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"8qyxhAClDx7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [-1.76137951 -2.18523489 -1.67534315 -1.41157512 -1.25974682 -0.1662507\n"," -1.44803074 -2.38230977]\n","Max:  [ 5.80880117  1.86027551 15.87559968 19.68483257 30.34662832  1.13302251\n","  2.95419426  2.50034395]\n","Mean:  [-0.0052895   0.02410787 -0.01402547 -0.00921717 -0.00730772 -0.01534856\n"," -0.0157594   0.00603172]\n","Std:  [0.97246906 0.99414832 0.75592352 0.72070377 1.01079355 0.06874798\n"," 1.00087568 0.9973681 ]\n"]}],"source":["describe(X_test_standardized)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"_tTsEozJ2_3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [0. 0. 0. 0. 0. 0. 0. 0.]\n","Max:  [1. 1. 1. 1. 1. 1. 1. 1.]\n","Mean:  [0.23267338 0.54016296 0.03256701 0.0226707  0.04988801 0.0019599\n"," 0.3296447  0.47313988]\n","Std:  [0.13209724 0.2471876  0.01878802 0.01518553 0.03951824 0.00997908\n"," 0.22691646 0.20070643]\n"]}],"source":["describe(X_train_normalized)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"bNplCpND3Byu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min:  [0. 0. 0. 0. 0. 0. 0. 0.]\n","Max:  [1. 1. 1. 1. 1. 1. 1. 1.]\n","Mean:  [0.23197465 0.54612213 0.09465689 0.06647378 0.03962615 0.1161435\n"," 0.32535169 0.48914824]\n","Std:  [0.12846048 0.24574114 0.04307025 0.03416239 0.03198069 0.05291264\n"," 0.22735678 0.20426763]\n"]}],"source":["describe(X_test_normalized)"]},{"cell_type":"markdown","metadata":{"id":"EgG7BAsDOx8B"},"source":["### 2. Linear regression"]},{"cell_type":"markdown","metadata":{"id":"aDLWBj56D6P6"},"source":["Now you are ready to put your dataset to train a model. You will continue to use the linear regression that you have made in the lab using the normal equation. \n","\n","- Create the **solver** function that creates a linear regression line and return the coefficents. You can re-use the function from the first lab.\n","- Here you should use **all available features** of the dataset.\n","- You should add one column representing a bias to your feature matrix.\n","\n","The normal equation can be represented as follows:\n","\n","$\\theta = (\\textbf{X}^T \\cdot \\textbf{X})^{-1} \\cdot \\textbf{X}^T \\cdot \\textbf{y}$"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"6EkjV6APOx8C"},"outputs":[],"source":["def solver(X, y):\n","  \"\"\"\n","  Get the weights and bias of linear regression classifier on the input dataset (X, y).\n","\n","  Input: \n","   - X: a set of features\n","   - y: labels\n","  Output:\n","   - theta: weights and bias of the linear regression\n","  \"\"\"\n","  X= np.hstack((np.ones((X.shape[0], 1)), X))\n","  XtX= np.dot(X.T, X)\n","  XtX_inv= np.linalg.inv(XtX)\n","  XtY= np.dot(X.T, y)\n","  theta= np.dot(XtX_inv, XtY)\n","  return theta"]},{"cell_type":"markdown","metadata":{"id":"xWZhZW4rExeQ"},"source":["You should run this solver function only on the standardized training set (**X_train_standardized**, **y_train**) to create the model and evalute it later on the test set.\n","\n","- Run the **solver** function on **X_train_standardized** and **y_train** and save the result to **theta**."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_8ETLDgE_5p","outputId":"390908bc-5d45-4e55-f9c3-5b043bc2f0db"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 2.06760962  0.82395769  0.11691602 -0.25921941  0.30324751 -0.01125326\n"," -0.03990851 -0.91406614 -0.88520051]\n"]}],"source":["theta = solver(X_train_standardized, y_train)\n","print(theta)"]},{"cell_type":"markdown","metadata":{"id":"dQDfYDDxF9o5"},"source":["You now have a complete model trained on the training set. Then the next interesting thing is to evaluate if the model is good enough by using the test set. To do this, you need to create a predict function that can return the expected value. \n","\n","- Create the **predict** function which put each instance into the regression equation to predict the value. **DO NOT USE ANY LOOP**."]},{"cell_type":"code","execution_count":22,"metadata":{"id":"p-JxOI09ledN"},"outputs":[],"source":["def predict(X, theta):\n","  \"\"\"\n","  Input: \n","   - X: data instances to predict\n","   - theta: trained regression coefficients\n","\n","  Output:\n","   - y_hat: predicted values (X @ weight) + bias\n","  \"\"\"\n","  X= np.hstack((np.ones((X.shape[0], 1)), X))\n","\n","  y_hat= np.dot(X, theta)\n","  return y_hat"]},{"cell_type":"markdown","metadata":{"id":"psp-L1wGRslu"},"source":["This predict function should be able to return the predicted value of the housing price. Then now you might want to return the mean squared error (and its variants) of the whole model. There can be many different metrics but here you will measure the mean squared error (MSE). MSE can be calculated as follows: "]},{"cell_type":"markdown","metadata":{"id":"ljB3uUqjWPbw"},"source":["$MSE = {\\frac{1}{n}\\sum_{t=1}^{n}(\\hat{y}_t - y_t)^2} $.\n","\n","Note that $\\hat{y}$ is a predicted label and $y$ is a true label."]},{"cell_type":"markdown","metadata":{"id":"aMTnVQGPWfBv"},"source":["- Create a function **rooted_mean_squared_error** that calculates the RMSE value."]},{"cell_type":"code","execution_count":23,"metadata":{"id":"laAN9bTMOx8C"},"outputs":[],"source":["def mean_squared_error(X, y, theta):\n","  \"\"\"\n","  Input:\n","    - X_test: data instances to test.\n","    - y_test: true values of the corresponding data instances (X_test).\n","    - theta: trained regression coefficients.\n","\n","  Output:\n","    - MSE: A MSE score.\n","\n","  Use predict function to calculate our predicted values.\n","  \"\"\"\n","  y_pred= predict(X, theta)\n","  mse= np.mean((y_pred - y) ** 2)\n","  return mse"]},{"cell_type":"markdown","metadata":{"id":"TReu0cnIWiz2"},"source":["- Run MSE function to test our model created by our solver function. Use predict function to calculate our predicted values. Report the MSE value. Since you used the standardized dataset for training, you should do the same for testing."]},{"cell_type":"markdown","metadata":{"id":"52aH-ML6Seu7"},"source":["There is another score called mean absolute percentage error (MAPE). MAPE is the percentage equivalent of MAE. Each residual is scaled against the actual value. MAPE has a clear interpretation since percentages are easier for \n","people to conceptualize.\n","\n","MAPE can be calculated as follows:\n","\n","$MAPE = \\frac{100\\%}{n}\\sum_{t=1}^{n}|\\frac{y_t - \\hat{y}_t}{y_t}|$\n","\n","- Implement a function for MAPE **mean_absolute_percentage_error**, which receives the same parameters *X*, *y*, and *theta*."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"uFEGAF7fqqeg"},"outputs":[],"source":["def mean_absolute_percentage_error(X, y, theta):\n","  \"\"\"\n","  Input:\n","    - X_test: data instances to test.\n","    - y_test: true values of the corresponding data instances (X_test).\n","    - theta: trained regression coefficients.\n","\n","  Output:\n","    - MAPE: a MAPE score.\n","\n","  Use predict function to calculate our predicted values.\n","  \"\"\"\n","  y_pred = predict(X, theta)\n","  mape= np.mean(np.abs((y - y_pred)/ y))* 100\n","  return mape"]},{"cell_type":"markdown","metadata":{"id":"PNC0hszHqyIP"},"source":["Train your regression model on the **standardized** training set and evaluate your method with two different scores: MSE and MAPE. Print two scores here."]},{"cell_type":"code","execution_count":25,"metadata":{"id":"wm8gHjCSq64P"},"outputs":[],"source":["mse_score = mean_squared_error(X_test_standardized, y_test, theta) \n","mape_score = mean_absolute_percentage_error(X_test_standardized, y_test, theta)"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWaJtiItrmDk","outputId":"ba786a85-0aad-45b1-9c6e-57b1a7be2cba"},"outputs":[{"data":{"text/plain":["(0.5227782996923129, 31.816446609071342)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["mse_score, mape_score"]},{"cell_type":"markdown","metadata":{"id":"royxfFHjRjw2"},"source":["### 4. Linear regression with regularization"]},{"cell_type":"markdown","metadata":{"id":"wphcDNvvkwL0"},"source":["You have learned the Ridge regression in the lecture. Fortunately, the Ridge regression also can be represented as a closed form solution with the normal equation. \n","\n","Your task here is to create a variant of your previous solver function supporting the Ridge regression. \n","\n","A closed form solution to Ridge can be represented as follows:\n","\n","$\\theta = (\\textbf{X}^T \\cdot \\textbf{X} + \\lambda \\textbf{I})^{-1} \\cdot \\textbf{X}^T \\cdot \\textbf{y}$\n","\n","where $\\textbf{I}$ is an $(n+1) \\times (n+1) $ identity matrix, since the feature matrix also includes the bias column.\n"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"jvYlCgotRlcK"},"outputs":[],"source":["def solver_with_ridge(X, y, alpha):\n","  \"\"\"\n","  Get the weights and bias of the linear regression line on the dataset X, using the labels y.\n","\n","  Input: \n","   - X: a set of features to get weights\n","   - y: class labels\n","  Output:\n","   - theta: weights and bias of the ridge regression\n","  \"\"\"\n","  X= np.insert(X, 0, 1, axis=1)\n","  n_features= X.shape[1]\n","\n","  A= np.dot(X.T, X) + alpha * np.identity(n_features)\n","  B= np.dot(X.T, y)\n","  theta= np.linalg.solve(A,B)\n","\n","  return theta"]},{"cell_type":"markdown","metadata":{"id":"G-LrDqWSmaGW"},"source":["Here, compare the performances changing the $\\lambda$ value. Use the $\\lambda$ value from 0 to 100 in increments of 0.5. Use RMSE as a score metric. Save those 300 scores into the list `scores`. \n","\n","- To iterate different $\\lambda$s, you can use a loop for your convenience."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"Vd9pJ8zUo_7u"},"outputs":[],"source":["scores = []\n","for alpha in np.arange(0, 100, 0.5):\n","    theta= solver_with_ridge(X_train_standardized, y_train, alpha)\n","    mse= mean_squared_error(X_test_standardized, y_test, theta)\n","    scores.append(mse)"]},{"cell_type":"markdown","metadata":{"id":"ILjT1k5FpLVe"},"source":["Plot the graph of different scores here. If you saved all scores in the list `scores`, you can simply run the block below. The resulting plot behaves in a different way based on your split training and test sets. Sometimes, the error just decreases or increases, but you can also see that the error decreases first, but after some point, it starts to increase. If you are interested, repeat many times to check different plots and you can even change the range from [0, 100] to something else. Uncomment the block below!"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"--Ky0LHO0wqM"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["plt.plot(np.arange(0, 100, 0.5), scores)\n","plt.xlabel(\"alpha\")\n","plt.ylabel(\"MSE\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S3nHFnagh0nC"},"source":["### 5. Model validation"]},{"cell_type":"markdown","metadata":{"id":"p7MnVQ5Rryof"},"source":["So far, you simply had one test set and one training set. Now the question is if those sets were enough to represent the dataset's distribution. To overcome this problem, various validation methods have been developed and used such as cross-validation or a repeated holdout test. Here, you will develop one function that performs the repeated holdout test. The key strategy of it is to create a completely different training and test set pair for each iteration. You need to iterate the holdout test that you performed many (k) times and return the average score.\n","\n","* You are allowed to use a for loop to iterate k different tests. However, you are not allowed to use the loop to create different indices to divide the dataset.\n","* You can call the `train_test_split` function you have developed above if it helps your development process."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"M7JEfZjGijfH"},"outputs":[],"source":["def repeated_hold_out(X, y, k, test_ratio):\n","  \"\"\"\n","  Input:\n","    - X: features\n","    - y: labels\n","    - test_ratio: ratio of the test set\n","  Output:\n","    - score: the average of k different test scores\n","  \n","  1. Iterate k times to perform k validation processes.\n","  2. For each iteration, split the dataset into training and test sets with *random* indices.\n","   - Note that each iteration should create different training and test sets.\n","  3. Use *standardization* to fix the scale of the dataset, you should only use the training set's properties.\n","  4. Fit your model with *solver* (without ridge) on the training set.\n","  5. Save your *RMSE* score into the list *scores*\n","  6. After all the iterations, return the average of *scores*.\n","\n","  \"\"\"\n","  n = X.shape[0]\n","  scores = []\n","  for i in range(k):\n","    indices = np.random.permutation(n)\n","    test_size= int(n * test_ratio)\n","    test_indices = indices[:test_size]\n","    train_indices = indices[test_size:]\n","\n","    X_train, X_test = X[train_indices], X[test_indices]\n","    y_train, y_test = y[train_indices], y[test_indices]\n","\n","    meann= X_train.mean(axis=0)\n","    std= X_train.std(axis=0)\n","    X_train = (X_train - meann) / std\n","    X_test = (X_test - meann) / std\n","\n","    beta= np.linalg.lstsq(X_train, y_train, rcond=None)[0]\n","    y_pred = np.dot(X_test, beta)\n","    score= np.mean((y_test - y_pred)**2)\n","    scores.append(score)\n","  return np.mean(scores)"]},{"cell_type":"markdown","metadata":{"id":"Po5UX-gb2idF"},"source":["Run repeated_hold_out on X and y with the test ratio = 0.3. Report the holdout score."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"QuMGeSGfLjAL"},"outputs":[{"data":{"text/plain":["4.838003606095827"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["holdout_score = repeated_hold_out(X, y, k=5, test_ratio= 0.3)\n","holdout_score"]},{"cell_type":"markdown","metadata":{"id":"uqr-5g3HBo6n"},"source":["- You have also learned k-fold cross validation which uses k different parts of the original datasets for training and validation. \n","- You first need to divide the dataset into k parts. Iterate those chunks (so, k times) and use each part as a test set.\n","- You need to merge the other k-1 sets and use it as a training set.\n","- Finally you should have the dataset split into two parts: one of size (k-1)/k * original size as a training set, and the other one of size 1/k * original size as a test set. Test set should be mutually exclusive - it should not have the same instance for each instance.\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"j3GoCvNZBl_I"},"outputs":[],"source":["def kfold(X, y, k = 10, shuffle=True):\n","  \"\"\"\n","  Input:\n","    - X: features.\n","    - y: labels.\n","    - k: the number of folds.\n","    - shuffle: a parameter that controls suffling indices\n","  Output:\n","    - score: the average of k different test scores. \n","\n","  1. Check if the suffle parameter is True:\n","    if True: Create randomly shuffled indices of the size of the data instances.\n","    else: Do not take any action. \n","  2. Divide the dataset into k parts. \n","  3. Iterate those chunks (so, k times). For each iteration, create the training and test sets with *random* indices.\n","   - Use each part of k chunks as a test set.\n","   - You need to merge the other k-1 sets and use it as a training set.\n","   - Note that each iteration should create different training and test sets.\n","  4. Use *normalization* to fix the scale of the dataset, you should only use the training set's properties.\n","  5. Fit your model with *solver* (without ridge) on the training set.\n","  6. Save your *MAPE* score into the list *scores*\n","  7. After all the iterations, return the average of *scores*.\n","  \"\"\"\n","  scores = []\n","  n = X.shape[0]\n","  if shuffle:\n","    indices = np.random.permutation(n)\n","    X= X[indices]\n","    y= y[indices]\n","  split_size= n // k\n","  \n","  for i in range (k):\n","    test_start, test_end= i * split_size, (i+1) * split_size\n","    X_test, y_test= X[test_start:test_end], y[test_start:test_end]\n","    X_train, y_train= np.concatenate((X[:test_start], X[test_end:])), np.concatenate((y[:test_start], y[test_end:]))\n","\n","    X_train_mean= np.mean(X_train, axis=0)\n","    X_train_std= np.std(X_train, axis=0)\n","    X_train_norm= (X_train - X_train_mean) / X_train_std\n","    X_test_norm= (X_test - X_train_mean) / X_train_std\n","\n","    w= np.linalg.lstsq(X_train_norm, y_train, rcond=None)[0]\n","    y_pred = X_test_norm.dot(w)\n","\n","    score= np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n","    scores.append(score)\n","\n","    return np.mean(scores)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["124.89620558914703\n"]}],"source":["scores= kfold( X, y, k=10, shuffle=True)\n","print(scores)"]},{"cell_type":"markdown","metadata":{"id":"D7mSIvYbRoQq"},"source":["### 6. Put things together"]},{"cell_type":"markdown","metadata":{"id":"DWjft22LHEJm"},"source":["It's time to put everything you have done together here. Here you will create a function that manages whole process from receiving raw datasets to returning performance metrics, by modifying the repeated holdout function. This will help you manage your process clearly since it must contain all the functions you use for your dataset (Later you will replace it with scikit-learn's pipeline technique for the same purpose) - By having these management functions, you can switch off some of the techniques, add more techniques in the middle, or replace some of them with other methods, without any problem or confusion.\n","\n","* Complete `pipeline` following the instruction."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"6CU_kV0DRpYg"},"outputs":[],"source":["def pipeline(X, y, k = 5, test_ratio = 0.2, norm_method = \"standardization\", eval_method = \"MSE\", alpha = 0):\n","  \"\"\"\n","  Input:\n","    - X: features\n","    - y: labels\n","    - test_ratio: ratio of the test set\n","  Output:\n","    - score: the average of k different test scores\n","  \n","  1. Iterate k times to perform k validation processes.\n","  2. For each iteration, split the dataset into the training and test sets with *random* indices.\n","   - Note that each iteration should create different training and test sets.\n","  3. Check the parameter *norm_method*\n","    - if norm_method == standardization:\n","      - Use *standardization* to fix the scale of the dataset, you should only use the training set's properties.\n","    - if norm_method == normalization:\n","      - Use *normalization* to fix the scale of the dataset, you should only use the training set's properties.\n","  4. Fit your model with *solver_with_ridge\" on the training set. Use alpha from the parameter.\n","  5. Check the parameter \"eval_method\"\n","    - if eval_method == \"MSE\"\n","      - Save your *MSE* score into the list *scores*\n","    - if eval_method == \"MAPE\"\n","      - Save your *MAPE* score into the list *scores*\n","\n","  6. After all the iterations, return the average of *scores*.\n","\n","  \"\"\"\n","  scores = []\n","  i =0\n","  while i<k:\n","    X_train,X_test,y_train,y_test=train_test_split(X,y, test_ratio)\n","\n","  if norm_method == \"standardization\":\n","    X_train, X_test = apply_standardization(X_train, X_test)\n","  elif norm_method == \"normalization\":\n","    X_train, X_test= apply_normalization(X_train, X_test)\n","  else:\n","    print(\"invalid method\")\n","    return None\n","  theta= solver_with_ridge(X_train, y_train, alpha)\n","    \n","  if eval_method == \"MSE\":\n","    rmse= mean_squared_error(X_test,y_test,theta)\n","    scores.append(rmse)\n","  elif eval_method == 'MAPE':\n","    mae= mean_absolute_percentage_error(X_test,y_test,theta)\n","    scores.append(mae)\n","  else:\n","    print(\"invalid method\")\n","    return None\n","  i+=1\n","  averagescore= np.mean(scores)\n","  return averagescore  "]},{"cell_type":"markdown","metadata":{"id":"P7GKrthoLOlD"},"source":["Now you are ready to run various tasks by using this single function. Will the best model the same under MSE or MAPE? Will different k or test ratio result in different best model? You can do many different trials to find a good model.\n","\n","- Change a normalization method, an alpha value to find out the best classifier under either MSE or MAPE score. This task is **mandatory**.\n","- Report at least five different trials under different settings.\n","- This is not an optional task for retake homework."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"bkCeCDoPLNGp"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn [37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m results\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m----> 2\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormalization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(score)\n\u001b[1;32m      4\u001b[0m score \u001b[38;5;241m=\u001b[39m pipeline(X, y, norm_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandardization\u001b[39m\u001b[38;5;124m\"\u001b[39m, eval_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmape\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n","Cell \u001b[0;32mIn [34], line 31\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(X, y, k, test_ratio, norm_method, eval_method, alpha)\u001b[0m\n\u001b[1;32m     29\u001b[0m i \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i\u001b[38;5;241m<\u001b[39mk:\n\u001b[0;32m---> 31\u001b[0m   X_train,X_test,y_train,y_test\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstandardization\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     34\u001b[0m   X_train, X_test \u001b[38;5;241m=\u001b[39m apply_standardization(X_train, X_test)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["results=[]\n","score = pipeline(X, y, norm_method=\"normalization\", eval_method=\"mse\", alpha=0.2)\n","results.append(score)\n","score = pipeline(X, y, norm_method=\"standardization\", eval_method=\"mape\", alpha=0.4)\n","results.append(score)\n","score = pipeline(X, y, norm_method=\"normalization\", eval_method=\"mse\", alpha=0.6)\n","results.append(score)\n","score = pipeline(X, y, norm_method=\"standardization\", eval_method=\"mape\", alpha=0.8)\n","results.append(score)\n","score = pipeline(X, y, norm_method=\"normalization\", eval_method=\"mse\", alpha=1)\n","results.append(score)\n","best_score = max(results)\n","best_score"]},{"cell_type":"markdown","metadata":{"id":"cWugHCgB2Cpq"},"source":["# END"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
